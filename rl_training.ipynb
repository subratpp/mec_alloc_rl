{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training RL Agent for given Number of Users for a single Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data for training\n",
    "The dataset was generated using codes available in dataset_generator folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'dataset/dual_s_data.csv'\n",
    "users_low = 100\n",
    "users_res = 100\n",
    "users_high = 500\n",
    "#number_of_service = 2\n",
    "\n",
    "#network latency\n",
    "N_lat = 0.25\n",
    "\n",
    "latency_threshold = 10 - N_lat #subtract latency due to network from total latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. states: RAM(MB), #Cores, BG WL(%), GPU(MB), S1:Users, S2:Users\n",
    "\n",
    "2. More reward for the number closer to (s1,s2) with latency below given threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "\n",
    "class yolosystem(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, n_actions, filename):\n",
    "        \n",
    "        super(yolosystem, self).__init__()\n",
    "        \n",
    "        self.n_actions = n_actions #total number of action space after ranging [10, 20, 30 ...]\n",
    "        self.action_space = spaces.Discrete(self.n_actions) #total number of users in the action space; starts with zero\n",
    "        self.observation_space = spaces.Box(low=np.array([0,0,0,0,0,0]), high=np.array([11000]*6), shape=(6, ), dtype=np.int32) #<RAM, Core, Workload>\n",
    "        self.seed()\n",
    "        self.current_obs = np.array( [3000, 2, 40, 2, 100, 100] ) #current observation = <ram, cores, workload%>\n",
    "\n",
    "        #Load dataset\n",
    "        self.df = pd.read_csv(filename)\n",
    "        # computer percentage of GPU usage from actual use\n",
    "        self.df['workload_gpu'] = self.df['workload_gpu'].multiply(1/80).round(0).astype(int) #round gpu workload\n",
    "\n",
    "        #get unique data in set\n",
    "        self.ram = self.df.ram.unique()\n",
    "        self.cores = self.df.cores.unique()\n",
    "        self.workload_cpu = self.df.workload_cpu.unique()\n",
    "        print(self.df) #print dataset\n",
    "       \n",
    "        \n",
    "\n",
    "    def seed(self, seed=1010):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action) #action should be in action space\n",
    "        state = self.current_obs\n",
    "        done = True #Episodes ends after each action\n",
    "\n",
    "        #compute latecy from the number of users\n",
    "        reward = self.get_reward(state, action) #linear latency           \n",
    "#         print(action, reward)\n",
    "        self.current_obs = self.get_random_state() #go to a random state\n",
    "        \n",
    "#         print(self.current_obs)\n",
    "        return self.current_obs, reward, done, {} #no-states, reward, episode-done, no-info\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_obs = self.get_random_state()\n",
    "        return self.current_obs #current state of the system with no load\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        print(f\"Current State:<{self.current_obs}>\")\n",
    "        \n",
    "    \n",
    "    #compute latency\n",
    "    def get_reward(self, state, action):\n",
    "        #change action to users\n",
    "        \n",
    "        u1 = action//5 + 1\n",
    "        u2 = (action+1) - (u1-1)*5\n",
    "        #sample time from dataframe\n",
    "        gram = state[0]\n",
    "        gcores = state[1]\n",
    "        gwl_c = state[2]\n",
    "        gwl_g = state[3]\n",
    "        gs1 = u1*100\n",
    "        gs2 = u2*100\n",
    "#         print(\"user:\", gs1, gs2, \"act:\", action)\n",
    "\n",
    "        fetch_state = self.df.loc[ (self.df['ram'] == gram) & (self.df['cores']== gcores) & (self.df['workload_cpu']==gwl_c) & (self.df['workload_gpu']==gwl_g) & (self.df['users_yolo']==gs1) & (self.df['users_mnet']==gs2)]\n",
    "                \n",
    "        if fetch_state.empty:\n",
    "            return -20 \n",
    "\n",
    "        time1 = fetch_state.sample().iloc[0]['time_yolo'] #fetch time from the dataframe\n",
    "        time2 = fetch_state.sample().iloc[0]['time_mnet']\n",
    "        tm = max(time1, time2)\n",
    "        #add total latencies due to network based on number of u1 and u2\n",
    "        \n",
    "        if (tm <= latency_threshold): \n",
    "            return  0.01*(gs1 - state[4]) +  0.01*(gs2 - state[5]) + u1 + u2 \n",
    "\n",
    "        else:\n",
    "            return -5 - u1 - u2     \n",
    "        \n",
    "    \n",
    "    #get to some random state after taking an action\n",
    "    def get_random_state(self):\n",
    "        #generate state randomly\n",
    "        gram = np.random.choice(self.ram, 1)[0]\n",
    "        gcores = np.random.choice(self.cores, 1)[0]\n",
    "        gwl_c = np.random.choice(self.workload_cpu, 1)[0]\n",
    "        \n",
    "        #fetch gamma for the state\n",
    "        fetch_state = self.df.loc[ (self.df['ram'] == gram) & (self.df['cores']== gcores) & (self.df['workload_cpu']==gwl_c) ]\n",
    "        gwl_g = fetch_state.sample().iloc[0]['workload_gpu'] #fetch workload randmoly\n",
    "        \n",
    "        gs1 = random.randrange(50, 550, 50)\n",
    "        gs2 = random.randrange(50, 550, 50)\n",
    "        \n",
    "        return np.array( [gram, gcores, gwl_c, gwl_g, gs1, gs2] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RL Environment with Baseline3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ram  cores  workload_cpu  workload_gpu  users_yolo  users_mnet  \\\n",
      "0      3000      2            40             2         100         100   \n",
      "1      3000      2            40             2         100         200   \n",
      "2      3000      2            40             2         100         300   \n",
      "3      3000      2            40             2         100         400   \n",
      "4      3000      2            40             2         100         500   \n",
      "...     ...    ...           ...           ...         ...         ...   \n",
      "5995  11000      5            60            10         500         100   \n",
      "5996  11000      5            60            10         500         200   \n",
      "5997  11000      5            60            10         500         300   \n",
      "5998  11000      5            60            10         500         400   \n",
      "5999  11000      5            60            10         500         500   \n",
      "\n",
      "      time_yolo  time_mnet  \n",
      "0     15.215310  17.846291  \n",
      "1     15.477644  21.690610  \n",
      "2     15.443997  27.328535  \n",
      "3     15.530827  32.847595  \n",
      "4     16.192997  38.689456  \n",
      "...         ...        ...  \n",
      "5995  59.541115  18.187416  \n",
      "5996  59.768538  26.538005  \n",
      "5997  60.304642  37.682399  \n",
      "5998  60.350351  48.019194  \n",
      "5999  60.743214  57.865394  \n",
      "\n",
      "[6000 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = yolosystem(25, datafile )\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 11000, (6,), int32)\n",
      "Discrete(25)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7\n",
      "-8\n",
      "-9\n",
      "-10\n",
      "-11\n",
      "-8\n",
      "-9\n",
      "-10\n",
      "-11\n",
      "-12\n",
      "-9\n",
      "-10\n",
      "-11\n",
      "-12\n",
      "-13\n",
      "-10\n",
      "-11\n",
      "-12\n",
      "-13\n",
      "-14\n",
      "-11\n",
      "-12\n",
      "-13\n",
      "-14\n",
      "-15\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    t = env.get_reward([3000, 2, 40, 2, 500, 500], i)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "reward= -14\n",
      "Current State:<[5000.    2.   40.    6.  450.  450.]>\n",
      "Step 2\n",
      "reward= -11\n",
      "Current State:<[9000.    5.   40.   10.   50.  400.]>\n",
      "Step 3\n",
      "reward= -11\n",
      "Current State:<[7000.    2.   40.   10.   50.  500.]>\n",
      "Step 4\n",
      "reward= -8\n",
      "Current State:<[9000.    2.   40.   10.  350.  350.]>\n",
      "Step 5\n",
      "reward= -10\n",
      "Current State:<[7000.    2.   60.    2.  350.  250.]>\n",
      "Step 6\n",
      "reward= -12\n",
      "Current State:<[7000.    3.   40.    3.   50.  400.]>\n",
      "Step 7\n",
      "reward= -8\n",
      "Current State:<[3000.    5.   50.    3.  100.  250.]>\n",
      "Step 8\n",
      "reward= -8\n",
      "Current State:<[7000.    4.   60.    6.   50.  450.]>\n",
      "Step 9\n",
      "reward= -7\n",
      "Current State:<[3000.    3.   40.    2.  500.  250.]>\n",
      "Step 10\n",
      "reward= -14\n",
      "Current State:<[5000.    2.   40.    3.  250.  500.]>\n"
     ]
    }
   ],
   "source": [
    "n_steps = 10\n",
    "for step in range(n_steps):\n",
    "    print(\"Step {}\".format(step + 1))\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "    print('reward=', reward)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard for Training Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "import os\n",
    "# Create log dir\n",
    "log_dir = './agent_tensorboard/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env = Monitor(env, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# wrap it\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(MlpPolicy, env, verbose=0, tensorboard_log = log_dir, exploration_fraction=0.4, learning_starts=150000,  train_freq=30, target_update_interval=30000, exploration_final_eps=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = time.time()\n",
    "model.learn(total_timesteps=500000) #reset_num_timesteps=False\n",
    "end = time.time()\n",
    "training_time = end-begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(f\"edge_agent_thres10\")\n",
    "# model.save(f\"edge_agent_{latency_threshold}_lin\")\n",
    "# del model  # delete trained model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "# from stable_baselines3 import DQN\n",
    "# model = DQN.load(\"edge_agent_20_lin\")\n",
    "#return action and state\n",
    "#model.predict(np.array([2000, 4, 30]), deterministic=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
